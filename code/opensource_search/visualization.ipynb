{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code for all visualizations in one place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set style for charts\n",
    "style = 'seaborn-v0_8'  # 'ggplot' and 'seaborn-v0_8-colorblind' are also good\n",
    "plt.style.use(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data after appending all entries and removing duplicates\n",
    "df_raw = pd.read_csv('data/initial/data.csv')  # [4654, 25]\n",
    "\n",
    "# filter out no main language, no description and non-programming main language\n",
    "# df_filter1 = pd.read_csv('data/initial/data_filtered_initial.csv')  # [1576, 31]\n",
    "df_filter1 = pd.read_csv('data/test/data_filtered_appended_all.csv')  # [1576, 31], fixed error in some data\n",
    "\n",
    "# filter out archived and non-updated in 5 years [1179, 32] (index columns)\n",
    "df_filter2 = pd.read_csv('data/initial/data_filtered_pre_readme_fetch.csv')\n",
    "\n",
    "# appended all data and removed empty readmes and non-english descriptions\n",
    "df_filter3 = pd.read_csv('data/appended/final.csv')  # [1028, 30]\n",
    "\n",
    "# results of manual filtering by description\n",
    "df_description = pd.read_csv('data/manual/filter_by_description.csv')\n",
    "\n",
    "# results of manual filtering by readme\n",
    "df_readme = pd.read_csv('data/manual/filter_by_readme.csv', skiprows=1)\n",
    "\n",
    "# marked as framework\n",
    "df_frameworkds = pd.read_csv('data/frams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gh_search import *\n",
    "from langdetect import detect\n",
    "import validators\n",
    "\n",
    "def fill_na_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  # fill null values with base data\n",
    "  df['description'].fillna(value='', inplace=True)\n",
    "  df['homepage_url'].fillna(value='', inplace=True)\n",
    "  df['license'].fillna(value='UNLICENSED', inplace=True)\n",
    "  df['main_language'].fillna(value='NONE', inplace=True)\n",
    "  print('Filled N/A values for \"description\", \"homepage_url\", '\n",
    "               '\"license\" and \"main_language\" with base values')\n",
    "  return df\n",
    "\n",
    "\n",
    "def filter_out_nocode(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  n = df.shape[0]\n",
    "  df = df[df['main_language'] != 'NONE']\n",
    "  num_removed_lang = n - df.shape[0]\n",
    "  df = df[df['repo_size_kb'] >= 50]\n",
    "  num_removed_size = n - df.shape[0] - num_removed_lang\n",
    "  print(f'Removed {num_removed_lang}/{n} repos due not having a '\n",
    "              'recognized programming language as main language on GitHub')\n",
    "  print(f'Removed {num_removed_size}/{n - num_removed_lang} repos due not '\n",
    "              'having >= 50 kB of content')\n",
    "  return df\n",
    "\n",
    "\n",
    "def filter_out_nodesc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  n = df.shape[0]\n",
    "  df = df[df['description'] != '']\n",
    "  num_removed = n - df.shape[0]\n",
    "  print(f'Removed {num_removed}/{n} repos due to not having a '\n",
    "              'description')\n",
    "  return df\n",
    "\n",
    "\n",
    "def filter_out_nonprog(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  n = df.shape[0]\n",
    "  lang: LinguistData = LinguistData()\n",
    "\n",
    "  def lang_filter(language: str) -> bool:\n",
    "    accepted_non_prog_langs: list = ['CSS', 'Mermaid', 'Prisma',\n",
    "                                     'Riot', 'Svelte', 'Vue']\n",
    "    return (lang in accepted_non_prog_langs\n",
    "            or lang.is_programming_language(language))\n",
    "  df = df[df['main_language'].apply(lang_filter)]\n",
    "  num_removed = n - df.shape[0]\n",
    "  print(f'Removed {num_removed}/{n} repos due to not being written in a '\n",
    "              f'programming language recognized by GitHub')\n",
    "  return df\n",
    "\n",
    "\n",
    "def filter_out_archived(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  n = df.shape[0]\n",
    "  df = df[df['is_archived'] == False]\n",
    "  num_removed = n - df.shape[0]\n",
    "  print(f'Removed {num_removed}/{n} repos due to being archived')\n",
    "  return df\n",
    "\n",
    "\n",
    "def filter_out_notupdated(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  n = df.shape[0]\n",
    "  diff: pd.Timestamp = pd.Timestamp.today(tz='UTC') - pd.Timedelta(days=365 * 5)\n",
    "  df = df[pd.to_datetime(df['updated_at']) >= diff]\n",
    "  num_removed = n - df.shape[0]\n",
    "  print(f'Removed {num_removed}/{n} repos due to being updated in the '\n",
    "              'last 5 years')\n",
    "  return df\n",
    "\n",
    "\n",
    "def filter_out_noreadme(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  n = df.shape[0]\n",
    "  df = df.dropna(subset=['readme'], inplace=False)\n",
    "  num_removed = n - df.shape[0]\n",
    "  print(f'Removed {num_removed}/{n} repos due having no Readme')\n",
    "  return df\n",
    "\n",
    "\n",
    "def filter_out_emptyreadme(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  n = df.shape[0]\n",
    "\n",
    "  def empty_readme_filter(row) -> bool:\n",
    "    name = row['name']\n",
    "    readme = row['readme']\n",
    "    return not (readme.startswith(f'# {name}') and len(readme.split('\\n')) <= 2)\n",
    "  df = df[df.apply(empty_readme_filter, axis=1)]\n",
    "  num_removed = n - df.shape[0]\n",
    "  print(f'Removed {num_removed}/{n} repos due to having the generated '\n",
    "              f'README format')\n",
    "  return df\n",
    "\n",
    "\n",
    "def filter_out_nonenglish(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  n = df.shape[0]\n",
    "\n",
    "  def desc_lang_filter(text: str) -> bool:\n",
    "    try:\n",
    "      v = detect(text.strip())\n",
    "      if df.owner == 'funf-core':\n",
    "        print(f'CORE: {v}')\n",
    "      return v == 'en'\n",
    "    except:\n",
    "      print(f'Unable to detect language of \"{text}\"')\n",
    "      if validators.url(text.strip()):\n",
    "        print(f'\"{text}\" is an url, accepted as valid')\n",
    "        return True\n",
    "      return False\n",
    "  df = df[df['description'].apply(desc_lang_filter)]\n",
    "  num_removed = df.shape[0]\n",
    "  print(f'Removed {num_removed}/{n} repos due not having an English '\n",
    "              'description text')\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw\n",
    "\n",
    "df = fill_na_values(df)\n",
    "df = filter_out_nocode(df)\n",
    "df = filter_out_nodesc(df)\n",
    "df = filter_out_nonprog(df)\n",
    "df['num_issues'] = [0] * len(df)\n",
    "df['num_subscribers'] = [0] * len(df)\n",
    "df['num_contributors'] = [0] * len(df)\n",
    "df['languages'] = [[]] * len(df)\n",
    "df['readme'] = [''] * len(df)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# popularity analysis\n",
    "df = df_filter1\n",
    "pop_df = df[['num_stars', 'num_subscribers', 'num_forks',\n",
    "                     'has_issues', 'num_issues', 'is_archived']]\n",
    "print(pop_df.describe())\n",
    "for key in ['num_stars', 'num_subscribers', 'num_forks', 'num_issues']:\n",
    "  print(f'Median of \"{key}\":', np.median(pop_df[key].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# archived ratio\n",
    "fig, ax = plt.subplots()\n",
    "archived_labels = ['archived', 'active']\n",
    "archived_sizes = [len(df[df['is_archived'] == True]),\n",
    "                  len(df[df['is_archived'] == False])]\n",
    "ax.pie(archived_sizes, labels=archived_labels, autopct='%1.1f%%')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time difference analysis\n",
    "time_df = df[['created_at', 'updated_at']]\n",
    "time_df['created_at'] = time_df['created_at'].apply(pd.to_datetime)\n",
    "time_df['updated_at'] = time_df['updated_at'].apply(pd.to_datetime)\n",
    "time_df['difference'] = time_df['updated_at'] - time_df['created_at']\n",
    "\n",
    "print(time_df.describe())\n",
    "\n",
    "creation_dates = time_df['created_at'].to_list()\n",
    "print(min(creation_dates))\n",
    "print(max(creation_dates))\n",
    "creation_seconds = [ts.timestamp() for ts in creation_dates]\n",
    "print(pd.Timestamp.fromtimestamp(sum(creation_seconds) / len(creation_seconds)))\n",
    "tmp_df = pd.DataFrame({'timestamps': creation_dates})\n",
    "tmp_df['year'] = tmp_df['timestamps'].dt.year\n",
    "print(tmp_df.groupby('year').size())\n",
    "\n",
    "xs = list(range(2009, 2025, 1))\n",
    "ys = [1, 5, 11, 15, 21, 38, 109, 149, 127, 163, 165, 193, 194, 154, 194, 37]\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "axes[0].plot(xs, ys)\n",
    "axes[0].xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "axes[0].set_title('Amount of new EMA Repositories per Year')\n",
    "axes[0].set_ylabel('Count')\n",
    "time_df.difference.dt.days.hist()\n",
    "axes[1].set_title('Difference between Creation and latest Update')\n",
    "axes[1].set_xlabel('Days after Creation')\n",
    "axes[1].set_ylabel('Count')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub repository growth per year\n",
    "gh_developers_dict = {'2016': 5.8,\n",
    "                      '2017': 24,\n",
    "                      '2018': 31,\n",
    "                      '2019': 40,\n",
    "                      '2020': 56,\n",
    "                      '2021': 73.5,\n",
    "                      '2022': 94}\n",
    "gh_repos_dict = {'2016': 19.4,\n",
    "                 '2017': 67,\n",
    "                 '2018': 96,\n",
    "                 '2019': 44,\n",
    "                 '2020': 60,\n",
    "                 '2021': 115,\n",
    "                 '2022': 200}\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
    "axes = axes.flatten()\n",
    "ax = axes[0]\n",
    "xs = list(gh_developers_dict.keys())\n",
    "ys = list(gh_developers_dict.values())\n",
    "ax.bar(xs, ys)\n",
    "ax.set_ylabel('Number of Users [Million]')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_title('GitHub Users per Year')\n",
    "ax = axes[1]\n",
    "xs = list(gh_repos_dict.keys())\n",
    "ys = list(gh_repos_dict.values())\n",
    "ax.bar(xs, ys)\n",
    "ax.set_ylabel('Number of new Repositories [Million]')\n",
    "ax.set_xlabel('Creation Year')\n",
    "ax.set_title('New GitHub Repositories per Year')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = list(tmp_dict.keys())[7:14]\n",
    "ema_repos_per_year = np.array(list(tmp_dict.values())[7:14])\n",
    "gh_repos_per_year = np.array(list(gh_repos_dict.values()))\n",
    "millioner = lambda t: t * 1000000\n",
    "millioner_func = np.vectorize(millioner)\n",
    "ys = ema_repos_per_year/millioner(gh_repos_per_year)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(xs, ys)\n",
    "ax.set_ylabel('\"Ema Repositories\" to \"All Repositories\" Ratio')\n",
    "ax.set_xlabel('Creation Year')\n",
    "fig.suptitle('Proportion of EMA Repositories per Year')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potentials = df_description[df_description.classification == 'Potential']\n",
    "unrelateds = df_description[df_description.classification == 'Unrelated']\n",
    "\n",
    "checked_readmes = df_description[df_description.readme_check == True]\n",
    "missing_infos = df_description[df_description.missing_info == True]\n",
    "\n",
    "checked_potentials = potentials.merge(checked_readmes, on='url', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information pie charts\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "x = potentials.shape[0]\n",
    "y = unrelateds.shape[0]\n",
    "labels = [f'potential\\n   ({x})', f'unrelated\\n({y})    ']\n",
    "sizes = [x, y]\n",
    "axes[0].pie(sizes, labels=labels, autopct='%1.1f%%')\n",
    "axes[0].set_title('Classification Results')\n",
    "x = checked_readmes.shape[0]\n",
    "y = df_description.shape[0] - x\n",
    "labels = [f'needed\\n  ({x})', f'not needed\\n({y})     ']\n",
    "sizes = [x, y]\n",
    "axes[1].pie(sizes, labels=labels, autopct='%1.1f%%')\n",
    "axes[1].set_title('Readme Checking')\n",
    "x = checked_potentials.shape[0]\n",
    "y = potentials.shape[0] - x\n",
    "labels = [f'potential\\n    ({x})', f'unrelated\\n({y})    ']\n",
    "sizes = [x, y]\n",
    "axes[2].pie(sizes, labels=labels, autopct='%1.1f%%')\n",
    "axes[2].set_title('Classification of Checked Entries')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_apps = df_readme[df_readme.classification == 'EMA App']\n",
    "ema_frameworks = df_readme[df_readme.classification == 'EMA Framework']\n",
    "ema_tools = df_readme[df_readme.classification == 'EMA Tool']\n",
    "unrelateds = df_readme[df_readme.classification == 'Unrelated']\n",
    "\n",
    "checked_external = df_readme[df_readme.external_check == True]\n",
    "missing_infos = df_readme[df_readme.missing_info == True]\n",
    "\n",
    "checked_apps = ema_apps.merge(checked_external, on='url', how='inner')\n",
    "checked_frameworks = ema_frameworks.merge(checked_external, on='url', how='inner')\n",
    "checked_tools = ema_tools.merge(checked_external, on='url', how='inner')\n",
    "checked_unrelateds = unrelateds.merge(checked_external, on='url', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information pie charts\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "a = ema_apps.shape[0]\n",
    "b = ema_frameworks.shape[0]\n",
    "c = ema_tools.shape[0]\n",
    "d = unrelateds.shape[0]\n",
    "labels = [f'App ({a})', f'Framework ({b})',\n",
    "          f'Tool ({c})', f'Unrelated ({d})']\n",
    "sizes = [a, b, c, d]\n",
    "axes[0].pie(sizes, labels=labels, autopct='%1.1f%%')\n",
    "axes[0].set_title('Classification Results')\n",
    "x = checked_external.shape[0]\n",
    "y = df_readme.shape[0] - x\n",
    "labels = [f'Needed ({x})', f'Not needed ({y})     ']\n",
    "sizes = [x, y]\n",
    "axes[1].pie(sizes, labels=labels, autopct='%1.1f%%')\n",
    "axes[1].set_title('External Checking')\n",
    "a = checked_apps.shape[0]\n",
    "b = checked_frameworks.shape[0]\n",
    "c = checked_tools.shape[0]\n",
    "d = checked_unrelateds.shape[0]\n",
    "labels = [f'App ({a})', f'Framework ({b})',\n",
    "          f'Tool ({c})', f'Unrelated ({d})']\n",
    "sizes = [a, b, c, d]\n",
    "axes[2].pie(sizes, labels=labels, autopct='%1.1f%%')\n",
    "axes[2].set_title('Classification of Checked Entries')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_man = ema_apps\n",
    "df_app = df_filter3[df_filter3.url.isin(df_man.url)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# popularity analysis\n",
    "pop_df = df_app[['num_stars', 'num_subscribers', 'num_forks',\n",
    "                  'has_issues', 'num_issues', 'is_archived']]\n",
    "print(pop_df.describe())\n",
    "for key in ['num_stars', 'num_subscribers', 'num_forks', 'num_issues']:\n",
    "  print(f'Median of \"{key}\":', np.median(pop_df[key].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribers\n",
    "fig, ax = plt.subplots()\n",
    "ax = df_app.num_subscribers.value_counts().sort_index().plot(kind='bar', label='Data')\n",
    "ax.set_xlabel('Number of Subscribers')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.grid(True)\n",
    "for p in ax.patches:\n",
    "  ax.annotate(str(int(p.get_height())),\n",
    "              (p.get_x() + p.get_width() / 2.0, p.get_height() - 0.25),\n",
    "              ha='center', va='center',\n",
    "              xytext=(0, 10), textcoords='offset points')\n",
    "ax.axvline(x=df_app.num_subscribers.mean(), color='green', linestyle='--', label='Mean')\n",
    "ax.axvline(x=df_app.num_subscribers.median(), color='orange', linestyle='--', label='Median')\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribers\n",
    "fig, ax = plt.subplots()\n",
    "ax = df_app.num_stars.value_counts().sort_index().plot(kind='bar', label='Data')\n",
    "ax.set_xlabel('Number of Stargazers')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.grid(True)\n",
    "for p in ax.patches:\n",
    "  ax.annotate(str(int(p.get_height())),\n",
    "              (p.get_x() + p.get_width() / 2.0, p.get_height() - 0.25),\n",
    "              ha='center', va='center',\n",
    "              xytext=(0, 10), textcoords='offset points')\n",
    "ax.axvline(x=df_app.num_stars.mean(), color='green', linestyle='--', label='Mean')\n",
    "ax.axvline(x=df_app.num_stars.median(), color='orange', linestyle='--', label='Median')\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forks\n",
    "fig, ax = plt.subplots()\n",
    "ax = df_app.num_forks.value_counts().sort_index().plot(kind='bar', label='Data')\n",
    "ax.set_xlabel('Number of Forks')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.grid(True)\n",
    "for p in ax.patches:\n",
    "  ax.annotate(str(int(p.get_height())),\n",
    "              (p.get_x() + p.get_width() / 2.0, p.get_height() - 0.25),\n",
    "              ha='center', va='center',\n",
    "              xytext=(0, 10), textcoords='offset points')\n",
    "ax.axvline(x=df_app.num_forks.mean(), color='green', linestyle='--', label='Mean')\n",
    "ax.axvline(x=df_app.num_forks.median()+0.2, color='orange', linestyle='--', label='Median')\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2)\n",
    "n = df_app.shape[0]\n",
    "m = df_man.missing_info.values.sum()\n",
    "axes[0].pie([m, n-m], labels=[f'Missing info ({m})',\n",
    "                              f'Full info ({n-m})'], autopct='%1.1f%%')\n",
    "axes[0].set_title('Description & README')\n",
    "l = df_man.external_check.values.sum()\n",
    "axes[1].pie([l, n-l], labels=[f'Found info ({l})',\n",
    "                              f'Did not find info ({n-l})'], autopct='%1.1f%%')\n",
    "axes[1].set_title('Other Sources')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owner_counts = df_app.owner.value_counts()\n",
    "multiple_owners = owner_counts[owner_counts > 1].index.tolist()\n",
    "print(\"Owners with multiple rows:\", multiple_owners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app.owner_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_app[df_app.owner_type == 'Organization'].owner.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_man.has_publication.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2)\n",
    "axes[0].pie([17, 30], labels=[f'â‰¥1 publications ({17})',\n",
    "                              f'no publication ({30})'], autopct='%1.1f%%')\n",
    "axes[0].set_title('Linked Publications')\n",
    "\n",
    "axes[1].pie([8, 9], labels=[f'Users ({8})',\n",
    "                              f'Organizations ({9})'], autopct='%1.1f%%')\n",
    "axes[1].set_title('User Types')\n",
    "\n",
    "# TODO: Write\n",
    "# Master Student,\n",
    "# 2 Unknown but scientific repos\n",
    "# 2 No scientific\n",
    "# 1 PHD Student\n",
    "# 1 Professor\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emaapps = df_readme[df_readme.classification == 'EMA App']\n",
    "df_samplingstrategy = df_emaapps[['url', 'event-contingent',\n",
    "                                  'signal-contingent', 'continuous', 'none']]\n",
    "df_input = df_emaapps[['url', 'uses_diaries', 'uses_interviews',\n",
    "                       'uses_questionnaires', 'uses_mic', 'uses_cam',\n",
    "                       'uses_sensing', 'uses_other']]\n",
    "df_platforms = df_emaapps[['url', 'on_smartphone', 'on_smartwatch',\n",
    "                           'in_browser', 'on_other', 'OS']]\n",
    "df_interventions = df_emaapps[['url', 'has_app_notifications',\n",
    "                               'has_device_notifications']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked bar chart: which input methods are being used\n",
    "def get_use_nums(diaries=False, interviews=False, questionnaires=False,\n",
    "                 mic=False, cam=False, sensing=False, other=False):\n",
    "    if diaries:\n",
    "      k = 'uses_diaries'\n",
    "    elif interviews:\n",
    "       k = 'uses_interviews'\n",
    "    elif questionnaires:\n",
    "       k = 'uses_questionnaires'\n",
    "    elif mic:\n",
    "       k = 'uses_mic'\n",
    "    elif cam:\n",
    "       k = 'uses_cam'\n",
    "    elif sensing:\n",
    "       k = 'uses_sensing'\n",
    "    elif other:\n",
    "       k = 'uses_other'\n",
    "    n = df_input[df_input[k] == True].shape[0]\n",
    "    filter_condition = (df_input['uses_diaries'] == diaries) & \\\n",
    "                       (df_input['uses_interviews'] == interviews) & \\\n",
    "                       (df_input['uses_questionnaires'] == questionnaires) & \\\n",
    "                       (df_input['uses_mic'] == mic) & \\\n",
    "                       (df_input['uses_cam'] == cam) & \\\n",
    "                       (df_input['uses_sensing'] == sensing) & \\\n",
    "                       (df_input['uses_other'] == other)\n",
    "    n_only = df_input[filter_condition].shape[0]\n",
    "    return n_only, n - n_only\n",
    "\n",
    "\n",
    "n_diaries_only, n_diaries_partly = get_use_nums(diaries=True)\n",
    "n_interviews_only, n_interviews_partly = get_use_nums(interviews=True)\n",
    "n_questionnaires_only, n_questionnaires_partly = get_use_nums(questionnaires=True)\n",
    "n_mic_only, n_mic_partly = get_use_nums(mic=True)\n",
    "n_cam_only, n_cam_partly = get_use_nums(cam=True)\n",
    "n_sensing_only, n_sensing_partly = get_use_nums(sensing=True)\n",
    "n_other_only, n_other_partly = get_use_nums(other=True)\n",
    "\n",
    "xs = ('Diary', 'Interview', 'Questionnaire',\n",
    "      'Microphone', 'Camera', 'Sensing', 'Other')\n",
    "ys_only = [n_diaries_only, n_interviews_only, n_questionnaires_only,\n",
    "           n_mic_only, n_cam_only, n_sensing_only, n_other_only]\n",
    "ys_partly = [n_diaries_partly, n_interviews_partly, n_questionnaires_partly,\n",
    "             n_mic_partly, n_cam_partly, n_sensing_partly, n_other_partly]\n",
    "ys = {\n",
    "   'Only this method': ys_only,\n",
    "   'At least one other method': ys_partly\n",
    "}\n",
    "width = 0.5\n",
    "fig, ax = plt.subplots()\n",
    "r = range(len(xs))\n",
    "p1 = ax.bar(r, ys_only, width=width, label='Only this method')\n",
    "p2 = ax.bar(r, ys_partly, bottom=ys_only, width=width, label='At least one other method')\n",
    "for i in range(len(r)):\n",
    "    do_partly = False\n",
    "    if ys_only[i] > 0:\n",
    "      do_partly = True\n",
    "      ax.text(r[i], ys_only[i] / 2, str(ys_only[i]), ha='center', va='center', color='white')\n",
    "    if ys_partly[i] > 0 and do_partly:\n",
    "      ax.text(r[i], ys_only[i] + ys_partly[i] / 2, str(ys_partly[i]), ha='center', va='center', color='white')\n",
    "    ax.text(r[i], ys_only[i] + ys_partly[i], str(ys_only[i] + ys_partly[i]), ha='center', va='bottom')\n",
    "ax.set_xlabel('Method')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Data Input Methods')\n",
    "ax.set_xticks(r)\n",
    "ax.set_xticklabels(xs)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked bar chart: which sampling strategies are employed\n",
    "def get_use_nums(event=False, signal=False, continuous=False):\n",
    "    if event:\n",
    "      k = 'event-contingent'\n",
    "    elif signal:\n",
    "       k = 'signal-contingent'\n",
    "    elif continuous:\n",
    "       k = 'continuous'\n",
    "    n = df_samplingstrategy[df_samplingstrategy[k] == True].shape[0]\n",
    "    filter_condition = (df_samplingstrategy['event-contingent'] == event) & \\\n",
    "                       (df_samplingstrategy['signal-contingent'] == signal) & \\\n",
    "                       (df_samplingstrategy['continuous'] == continuous)\n",
    "    n_only = df_samplingstrategy[filter_condition].shape[0]\n",
    "    return n_only, n - n_only\n",
    "\n",
    "\n",
    "n_event_only, n_event_partly = get_use_nums(event=True)\n",
    "n_signal_only, n_signal_partly = get_use_nums(signal=True)\n",
    "n_continuous_only, n_continuous_partly = get_use_nums(continuous=True)\n",
    "\n",
    "xs = ('Event-Contingent', 'Signal-Contingent', 'Continuous')\n",
    "ys_only = [n_event_only, n_signal_only, n_continuous_only]\n",
    "ys_partly = [n_event_partly, n_signal_partly, n_continuous_partly]\n",
    "ys = {\n",
    "   'Only this strategy': ys_only,\n",
    "   'At least one other strategy': ys_partly\n",
    "}\n",
    "width = 0.5\n",
    "fig, ax = plt.subplots()\n",
    "r = range(len(xs))\n",
    "p1 = ax.bar(r, ys_only, width=width, label='Only this method')\n",
    "p2 = ax.bar(r, ys_partly, bottom=ys_only, width=width, label='At least one other method')\n",
    "for i in range(len(r)):\n",
    "    do_partly = False\n",
    "    if ys_only[i] > 0:\n",
    "      do_partly = True\n",
    "      ax.text(r[i], ys_only[i] / 2, str(ys_only[i]), ha='center', va='center', color='white')\n",
    "    if ys_partly[i] > 0 and do_partly:\n",
    "      ax.text(r[i], ys_only[i] + ys_partly[i] / 2, str(ys_partly[i]), ha='center', va='center', color='white')\n",
    "    ax.text(r[i], ys_only[i] + ys_partly[i], str(ys_only[i] + ys_partly[i]), ha='center', va='bottom')\n",
    "ax.set_xlabel('Strategy')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Sampling Strategies')\n",
    "ax.set_xticks(r)\n",
    "ax.set_xticklabels(xs)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=((9, 4)))\n",
    "m = df_interventions.shape[0] # total num of rows\n",
    "# num of app notifications locally\n",
    "n = df_interventions[df_interventions.has_app_notifications == True].shape[0]\n",
    "# num of device supported notificaions via other devices\n",
    "l = df_interventions[df_interventions.has_device_notifications == True].shape[0]\n",
    "cond = (df_interventions.has_app_notifications == True) | \\\n",
    "       (df_interventions.has_device_notifications == True)\n",
    "k = df_interventions[cond].shape[0]  # num apps that support any notification\n",
    "axes[0].pie([k, m-k], labels=[f'supported ({k})',\n",
    "                              f'not supported ({m-k})'], autopct='%1.1f%%')\n",
    "axes[0].set_title('Notifications')\n",
    "axes[1].pie([n, k-n], labels=[f'supported ({n})',\n",
    "                              f'\\n\\nnot supported ({k-n})'], autopct='%1.1f%%')\n",
    "axes[1].set_title('In-App Notifications')\n",
    "axes[2].pie([l, k-l], labels=[f'supported ({l})',\n",
    "                              f'not supported ({k-l})'], autopct='%1.1f%%')\n",
    "axes[2].set_title('Device Notifications')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all tech info for the 47 repos\n",
    "urls = df_emaapps.url.to_list()\n",
    "df_tech = df_filter3[df_filter3.url.isin(urls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# licensing\n",
    "license_dict = df_tech.license.value_counts().to_dict()\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(list(license_dict.values()),\n",
    "       labels=list(license_dict.keys()), autopct='%1.1f%%')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# languages in use\n",
    "fig, ax = plt.subplots()\n",
    "ax = df_tech.groupby('main_language').size().sort_values(ascending=False).plot(\n",
    "  kind='bar'\n",
    ")\n",
    "ax.set_xlabel('Main Language')\n",
    "ax.set_ylabel('Frequency')\n",
    "for p in ax.patches:\n",
    "  ax.annotate(str(int(p.get_height())),\n",
    "              (p.get_x() + p.get_width() / 2.0, p.get_height() - 0.2),\n",
    "              ha='center', va='center',\n",
    "              xytext=(0, 10), textcoords='offset points')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "lang_dict = {}\n",
    "for i in range(df_tech.shape[0]):\n",
    "  row = df_tech.iloc[i]\n",
    "  langs = [row.main_language] + literal_eval(row.languages)\n",
    "  for lang in langs:\n",
    "    if lang not in lang_dict.keys():\n",
    "      lang_dict[lang] = 1\n",
    "    else:\n",
    "      lang_dict[lang] += 1\n",
    "lang_dict = dict(sorted(lang_dict.items(), key=lambda e: e[1], reverse=True))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(list(lang_dict.keys())[:10], list(lang_dict.values())[:10])\n",
    "ax.set_xlabel('Used Language')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_xticklabels(list(lang_dict.keys())[:10], rotation=90)\n",
    "for p in ax.patches:\n",
    "  ax.annotate(str(int(p.get_height())),\n",
    "              (p.get_x() + p.get_width() / 2.0, p.get_height() - 0.4),\n",
    "              ha='center', va='center',\n",
    "              xytext=(0, 10), textcoords='offset points')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting times to be a datetime format\n",
    "df_tech.created_at = df_tech.created_at.apply(pd.to_datetime)\n",
    "df_tech.updated_at = df_tech.updated_at.apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_apps_gh.groupby([df_apps_gh.created_at.dt.year, df_apps_gh.main_language]).size()\n",
    "fig, ax = plt.subplots(ncols=3, nrows=2, figsize=(10, 6))\n",
    "\n",
    "ys_android = []\n",
    "ys_java = []\n",
    "ys_kotlin = []\n",
    "ys_iOS = []\n",
    "ys_swift = []\n",
    "ys_objectivec = []\n",
    "ys_other = []\n",
    "for year in range(2014, 2025, 1):\n",
    "  ts_langs_dict = {'Android': 0, 'Java': 0, 'Kotlin': 0, 'iOS': 0,\n",
    "                   'Swift': 0, 'Objective-C': 0, 'Other': 0}\n",
    "  df_year = df_tech[df_tech.created_at.dt.year == year]\n",
    "  for i in range(df_year.shape[0]):\n",
    "    if df_year.iloc[i].main_language in list(ts_langs_dict.keys()):\n",
    "      ts_langs_dict[df_year.iloc[i].main_language] += 1\n",
    "    else:\n",
    "      ts_langs_dict['Other'] += 1\n",
    "  ts_langs_dict['Android'] = ts_langs_dict['Java'] + ts_langs_dict['Kotlin']\n",
    "  ts_langs_dict['iOS'] = ts_langs_dict['Swift'] + ts_langs_dict['Objective-C']\n",
    "  ys_android.append(ts_langs_dict['Android'])\n",
    "  ys_java.append(ts_langs_dict['Java'])\n",
    "  ys_kotlin.append(ts_langs_dict['Kotlin'])\n",
    "  ys_iOS.append(ts_langs_dict['iOS'])\n",
    "  ys_swift.append(ts_langs_dict['Swift'])\n",
    "  ys_objectivec.append(ts_langs_dict['Objective-C'])\n",
    "  ys_other.append(ts_langs_dict['Other'])\n",
    "\n",
    "ax[0, 0].set_ylabel('By Year')\n",
    "ax[0, 0].plot(list(range(2014, 2025, 1)), ys_android, label='Android')\n",
    "ax[0, 0].plot(list(range(2014, 2025, 1)), ys_iOS, label='iOS')\n",
    "ax[0, 0].plot(list(range(2014, 2025, 1)), ys_other, label='Other')\n",
    "ax[0, 0].legend()\n",
    "\n",
    "ax[0, 1].plot(list(range(2014, 2025, 1)), ys_java, label='Java')\n",
    "ax[0, 1].plot(list(range(2014, 2025, 1)), ys_kotlin, label='Kotlin')\n",
    "ax[0, 1].legend()\n",
    "\n",
    "ax[0, 2].plot(list(range(2014, 2025, 1)), ys_swift, label='Swift')\n",
    "ax[0, 2].plot(list(range(2014, 2025, 1)), ys_objectivec, label='Objective-C')\n",
    "ax[0, 2].legend()\n",
    "\n",
    "ys_android = []\n",
    "ys_java = []\n",
    "ys_kotlin = []\n",
    "ys_iOS = []\n",
    "ys_swift = []\n",
    "ys_objectivec = []\n",
    "ys_other = []\n",
    "for year in range(2014, 2025, 1):\n",
    "  ts_langs_dict = {'Android': 0, 'Java': 0, 'Kotlin': 0, 'iOS': 0,\n",
    "                   'Swift': 0, 'Objective-C': 0, 'Other': 0}\n",
    "  df_year = df_tech[df_tech.created_at.dt.year <= year]\n",
    "  for i in range(df_year.shape[0]):\n",
    "    if df_year.iloc[i].main_language in list(ts_langs_dict.keys()):\n",
    "      ts_langs_dict[df_year.iloc[i].main_language] += 1\n",
    "    else:\n",
    "      ts_langs_dict['Other'] += 1\n",
    "  ts_langs_dict['Android'] = ts_langs_dict['Java'] + ts_langs_dict['Kotlin']\n",
    "  ts_langs_dict['iOS'] = ts_langs_dict['Swift'] + ts_langs_dict['Objective-C']\n",
    "  ys_android.append(ts_langs_dict['Android'])\n",
    "  ys_java.append(ts_langs_dict['Java'])\n",
    "  ys_kotlin.append(ts_langs_dict['Kotlin'])\n",
    "  ys_iOS.append(ts_langs_dict['iOS'])\n",
    "  ys_swift.append(ts_langs_dict['Swift'])\n",
    "  ys_objectivec.append(ts_langs_dict['Objective-C'])\n",
    "  ys_other.append(ts_langs_dict['Other'])\n",
    "\n",
    "ax[1, 0].set_ylabel('Cumulative')\n",
    "ax[1, 0].plot(list(range(2014, 2025, 1)), ys_android, label='Android')\n",
    "ax[1, 0].plot(list(range(2014, 2025, 1)), ys_iOS, label='iOS')\n",
    "ax[1, 0].plot(list(range(2014, 2025, 1)), ys_other, label='Other')\n",
    "ax[1, 0].legend()\n",
    "\n",
    "ax[1, 1].plot(list(range(2014, 2025, 1)), ys_java, label='Java')\n",
    "ax[1, 1].plot(list(range(2014, 2025, 1)), ys_kotlin, label='Kotlin')\n",
    "ax[1, 1].legend()\n",
    "\n",
    "ax[1, 2].plot(list(range(2014, 2025, 1)), ys_swift, label='Swift')\n",
    "ax[1, 2].plot(list(range(2014, 2025, 1)), ys_objectivec, label='Objective-C')\n",
    "ax[1, 2].legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# platforms\n",
    "'Android' in df_platforms.iloc[0].OS\n",
    "os_dict = {'Android': 0, 'iOS': 0, 'Cross-Plattform': 0, 'PC': 0}\n",
    "for i in range(df_platforms.shape[0]):\n",
    "  os = str(df_platforms.iloc[i].OS)\n",
    "  if 'Android' in os and ',' not in os:\n",
    "    os_dict['Android'] += 1\n",
    "  elif 'iOS' in os and ',' not in os:\n",
    "    os_dict['iOS'] += 1\n",
    "  elif 'Linux' in os or 'MacOS' in os or'Windows' in os:\n",
    "    os_dict['PC'] += 1\n",
    "  elif 'Android, iOS' in os or 'Any' in os:\n",
    "    os_dict['Cross-Plattform'] += 1\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2)\n",
    "ax[0].pie(list(os_dict.values()),\n",
    "          labels=list(os_dict.keys()), autopct='%1.1f%%')\n",
    "ax[0].set_title('Supported OS')\n",
    "ys = [df_platforms[df_platforms.on_smartphone == True].shape[0],\n",
    "      df_platforms[df_platforms.on_smartwatch == True].shape[0],\n",
    "      df_platforms[df_platforms.in_browser == True].shape[0],\n",
    "      df_platforms[df_platforms.on_other.notna()].shape[0]]\n",
    "ax[1].bar(['Smartphone', 'Smartwatch', 'Browser', 'Other'], ys)\n",
    "ax[1].set_title('Supported Device')\n",
    "for p in ax[1].patches:\n",
    "  ax[1].annotate(str(int(p.get_height())),\n",
    "                 (p.get_x() + p.get_width() / 2.0, p.get_height() - 0.6),\n",
    "                 ha='center', va='center',\n",
    "                 xytext=(0, 10), textcoords='offset points')\n",
    "ax[1].set_xticklabels(['Phone', 'Watch', 'Browser', 'Other'])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_platforms.on_other.replace(to_replace='PC', value=1, inplace=True)\n",
    "df_platforms.on_other.fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix\n",
    "corr_matrix = df_platforms[[\"on_smartphone\", \"on_smartwatch\",\n",
    "                            \"in_browser\", \"on_other\"]].corr()\n",
    "\n",
    "# Configure the heatmap plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(corr_matrix, cmap=\"coolwarm\")  # Choose a colormap\n",
    "\n",
    "# Add colorbar\n",
    "fig.colorbar(im, label=\"Correlation Coefficient\")\n",
    "\n",
    "# Set ticks and labels for x and y axes\n",
    "ax.set_xticks(range(len(corr_matrix.columns)))\n",
    "ax.set_yticks(range(len(corr_matrix.columns)))\n",
    "ax.set_xticklabels(corr_matrix.columns, rotation=45, ha=\"right\")  # Rotate x-axis labels\n",
    "ax.set_yticklabels(corr_matrix.columns)\n",
    "\n",
    "# Add labels for each cell (optional)\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "  for j in range(len(corr_matrix.columns)):\n",
    "    ax.text(j, i, f\"{corr_matrix.iloc[i, j]:.2f}\", ha=\"center\", va=\"center\")  # Format to 2 decimal places\n",
    "\n",
    "plt.xlabel(\"Device\")\n",
    "plt.ylabel(\"Device\")\n",
    "plt.title(\"Correlation Between Device Usage (Heatmap)\")\n",
    "plt.grid(False)\n",
    "plt.tight_layout()  # Adjust layout for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_platforms.copy()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "contents = {\n",
    "  'smartphone': [],\n",
    "  'smartwatch': [],\n",
    "  'browser': [],\n",
    "  'PC': []\n",
    "}\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "  row = df.loc[i]\n",
    "  if row.on_smartphone == True:\n",
    "    contents['smartphone'].append(i)\n",
    "  if row.on_smartwatch == True:\n",
    "    contents['smartwatch'].append(i)\n",
    "  if row.in_browser == True:\n",
    "    contents['browser'].append(i)\n",
    "  if row.on_other == True:\n",
    "    contents['PC'].append(i)\n",
    "\n",
    "from upsetplot import from_contents, plot\n",
    "plot(from_contents(contents))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(from_contents(contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_condition = ((df_platforms['on_smartwatch'] == True) & \\\n",
    "                    (df_platforms['on_other'] >= 0) & \\\n",
    "                    (df_platforms['in_browser'] == True) & \\\n",
    "                    (df_platforms['on_smartphone'] == True))\n",
    "df_platforms[filter_condition].shape[0]\n",
    "\n",
    "# 34 + 6 + 3 + 1 + 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches_dict = df_tech.default_branch.value_counts().to_dict()\n",
    "fig, axes = plt.subplots(ncols=3, figsize=((12, 3)))\n",
    "\n",
    "axes[0].pie(list(branches_dict.values()), labels=list(branches_dict.keys()),\n",
    "            autopct='%1.1f%%')\n",
    "axes[0].set_title('All Repositories')\n",
    "\n",
    "branches_dict = df_tech[df_tech.created_at.dt.year < 2020].default_branch.value_counts().to_dict()\n",
    "axes[1].pie(list(branches_dict.values()), labels=list(branches_dict.keys()),\n",
    "            autopct='%1.1f%%')\n",
    "axes[1].set_title('Before 2020')\n",
    "\n",
    "branches_dict = df_tech[df_tech.created_at.dt.year >= 2020].default_branch.value_counts().to_dict()\n",
    "axes[2].pie(list(branches_dict.values()), labels=list(branches_dict.keys()),\n",
    "            autopct='%1.1f%%')\n",
    "axes[2].set_title('After 2020')\n",
    "\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
